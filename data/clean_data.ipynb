{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the UNHCR dataset\n",
    "To clean the UNHCR data, I:\n",
    "- Drop duplicate rows for Antigua and Barbuda\n",
    "- Replace non contiguous values with contiguous values where necessary\n",
    "- Remove problematic observations based on SuddenREFROCDecreases.xlsx - provided by Geraldine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows\n",
    "df = pd.read_csv('raw/data.csv',engine='pyarrow').drop_duplicates(['Id','year']).drop('', axis=1)\n",
    "\n",
    "# replace contig values that should be ones but are zeros\n",
    "df.loc[df.Id.isin(['CODUGA','UGACOD','SDNSSD','SSDSDN','SSDETH','ETHSSD','SSDUGA',\n",
    "                   'UGASSD','CODCOG','COGCOD','CAFCOD','CODCAF','CODBDI','BDICOD',\n",
    "                   'SSDKEN','KENSSD','SSDCOD','CODSSD','CODZMB','ZMBCOD','AUTHUN','HUNAUT',\n",
    "                     'UVKALB','ALBUVK','MNEALB','ALBMNE','WBGISR','ISRWBG','JORWBG','WBGJOR',\n",
    "                     'BGRSRB','SRBGBR','ROUSRB','SRBROU','HUNSRB','SRBHUN']), 'contig'] = 1\n",
    "\n",
    "# sudan isn't contiguous with kenya after 2011\n",
    "# df.loc[df.Id.isin(['KENSDN','SDNKEN']), 'contig'] = 0 \n",
    "\n",
    "# remove problematic observations\n",
    "df.loc[(df.Id == 'CODAGO') & df.year.isin([2017, 2018]), 'newarrival'] = np.nan\n",
    "\n",
    "df.loc[df.Id == 'AFGIRN', 'newarrival']= np.nan\n",
    "\n",
    "df.loc[df.Id == 'AFGPAK', 'newarrival'] = np.nan\n",
    "\n",
    "df.loc[df.Id == 'ERTSDN', 'newarrival']= np.nan\n",
    "\n",
    "df.loc[(df.Id == 'SYRTUR') & (df.year > 2018), 'newarrival']= np.nan\n",
    "\n",
    "# any newarrivals after 2021 set to nan\n",
    "df.loc[df.year > 2021, 'newarrival'] = np.nan\n",
    "\n",
    "df.to_csv('clean/unhcr.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there are some countries which the distances are the exact same for all pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df.iso_o.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus_country_distances = []\n",
    "for country in countries:\n",
    "    if len(df.loc[df.iso_o == country, 'dist'].unique()) == 1:\n",
    "        sus_country_distances.append(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COD', 'UVK', 'MNE', 'SRB', 'SSD', 'WBG']"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sus_country_distances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Migration data\n",
    "This migration stock data comes from [Our World in Data](https://ourworldindata.org/migration#explore-data-on-where-people-migrate-from-and-to)\n",
    "It is every five years from 1990 to 2020.\n",
    "\n",
    "- I read the data in\n",
    "- Add 2021\n",
    "- Melt it to long format\n",
    "- I forward fill the data because these migration stocks are only released every five years.\n",
    "- Use country converter to convert names to iso 3 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw/migration-flows.csv', engine='pyarrow').rename({'Year':'year'},axis=1)\n",
    "df = df.loc[:,~df.columns.str.contains('_origin|_destination')]\n",
    "df = df[df.year >= 2000]\n",
    "\n",
    "df['year'] = pd.to_datetime(df.year, format='%Y')\n",
    "\n",
    "df_2021 = df[df.year == pd.to_datetime('2020', format='%Y')].copy()\n",
    "\n",
    "# change 2020 to 2021\n",
    "df_2021['year'] += pd.Timedelta(366, unit='d')\n",
    "\n",
    "# concatenate 2021 w/ rest\n",
    "df = pd.concat([df, df_2021], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I melt the data frame, and fill in missing years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt df, split Emigrants Immigrants columns, filter out rows with same origin and destination\n",
    "df = df.melt(id_vars=['Country','year'], var_name='Country_d').rename({'Country':'Country_o'}, axis=1).fillna(0)\n",
    "df[['EmigrantsImmigrants', 'Country_d']] =  df.Country_d.str.split(' from | to ', expand=True)\n",
    "df = df[(df.Country_d != df.Country_o)]\n",
    "\n",
    "# for values that say emigrants, switch the origin with destination\n",
    "emigrants_mask = df['EmigrantsImmigrants'] == 'Emigrants'\n",
    "df.loc[emigrants_mask, ['Country_o', 'Country_d']] = df.loc[emigrants_mask, ['Country_d', 'Country_o']].values\n",
    "\n",
    "df = df[['year', 'Country_o', 'EmigrantsImmigrants', 'Country_d', 'value']].drop('EmigrantsImmigrants', axis=1).drop_duplicates(['year','Country_o','Country_d'])\n",
    "df.value = df.value.abs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country long names get replaced with iso 3 short names using country converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caribbean not found in regex\n",
      "Channel Islands not found in regex\n",
      "Polynesia not found in regex\n"
     ]
    }
   ],
   "source": [
    "# convert the country names to iso3\n",
    "import country_converter as coco\n",
    "iso3_unique = coco.convert(names=df.Country_o.unique(), to='ISO3')\n",
    "iso_3_dict = dict(zip(df.Country_o.unique(), iso3_unique))\n",
    "\n",
    "df[['Country_o','Country_d']] = df[['Country_o','Country_d']].apply(lambda x: x.map(iso_3_dict))\n",
    "df.rename({'Country_o':'iso_o','Country_d':'iso_d', 'value':'migration_stock'}, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I convert stock data to the percent of migrants both from the perspective of the origin country and destination country.\n",
    "unhcr_data = pd.read_csv('clean/unhcr.csv', engine='pyarrow')[['iso_o','iso_d','year','pop_o','pop_d']].drop_duplicates(['iso_o','iso_d','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'year':'Year'}, inplace=True, axis=1)\n",
    "df['year'] = df.Year.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unhcr_data.merge(df, on=['iso_o','iso_d','year'], how='left')\n",
    "df[['pop_o','pop_d']] = df[['pop_o','pop_d']].apply(lambda x: x * 1000000)\n",
    "df[['migration_stock_pct_o','migration_stock_pct_d']] = df[['pop_o','pop_d']].apply(lambda x: df.migration_stock*100/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['pop_o','pop_d','Year'],axis=1).rename({'Year':'year'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing years.\n",
    "df = df.set_index(['year','iso_o','iso_d']).groupby(['iso_o', 'iso_d']).fillna(method='ffill')\n",
    "df.reset_index(inplace=True)\n",
    "df['years_since_migration_report'] = df.year % 5\n",
    "\n",
    "# The remaning two countries ('WBG' and 'UVK'), do mean imputation.\n",
    "df[['migration_stock', 'migration_stock_pct_o','migration_stock_pct_d']] = df[['migration_stock', 'migration_stock_pct_o','migration_stock_pct_d']].apply(lambda x: x.fillna(value = x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "df.to_csv('clean/migration_stocks.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning IDP data \n",
    "\n",
    "This IDP data on internally displaced people comes from the IDMC https://www.internal-displacement.org/database/displacement-data.\n",
    "\n",
    "It is yearly from 2008 to 2022, and contains data on both the stock of internally displace peoples and the new amount that year.\n",
    "\n",
    "As a prior, let's say that disaster idp is irrelevant to refugee newarrivals and drop disaster idp data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in an rename columns\n",
    "idp_df = pd.read_excel('raw/IDMC_Internal_Displacement_Conflict-Violence_Disasters.xlsx').rename({'Year':'year','Name':'Country','ISO3':'iso','Conflict Stock Displacement (Raw)':'conflict_stock_idp', 'Conflict Internal Displacements (Raw)':'conflict_idp',\n",
    "       'Disaster Internal Displacements (Raw)':'disaster_idp',\n",
    "       'Disaster Stock Displacement (Raw)':'disaster_stock_idp'}, axis=1).drop(['Country','Conflict Stock Displacement','Conflict Internal Displacements','Disaster Internal Displacements','Disaster Stock Displacement', \n",
    "       'disaster_stock_idp','disaster_idp'], axis=1)\n",
    "\n",
    "unhcr_data = pd.read_csv('raw/data.csv', engine='pyarrow')[['iso_o','year','pop_o']].drop_duplicates(['iso_o','year'])\n",
    "newarrival_o = pd.read_csv('raw/data.csv', engine='pyarrow').groupby(['iso_o','year']).newarrival.sum().reset_index()\n",
    "unhcr_data = unhcr_data.merge(newarrival_o, on=['iso_o','year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_idp = unhcr_data.merge(idp_df, how='left', left_on=['iso_o','year'], right_on=['iso','year']).drop('iso',axis=1)\n",
    "#merged_idp['disaster_pct'] = (merged_idp.disaster_idp/merged_idp.pop_o/1000000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant amount of data that are missing in the idp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iso_o                 0.000000\n",
       "year                  0.000000\n",
       "pop_o                 0.000000\n",
       "newarrival            0.000000\n",
       "conflict_stock_idp    0.852041\n",
       "conflict_idp          0.864694\n",
       "dtype: float64"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('proportion missing')\n",
    "merged_idp.isna().sum() / merged_idp.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we assume that this data is missing because the values should be zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_o</th>\n",
       "      <th>year</th>\n",
       "      <th>pop_o</th>\n",
       "      <th>newarrival</th>\n",
       "      <th>conflict_stock_idp</th>\n",
       "      <th>conflict_idp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>VEN</td>\n",
       "      <td>2018</td>\n",
       "      <td>28.903</td>\n",
       "      <td>2508056.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>VEN</td>\n",
       "      <td>2019</td>\n",
       "      <td>27.817</td>\n",
       "      <td>1524435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>VEN</td>\n",
       "      <td>2021</td>\n",
       "      <td>27.586</td>\n",
       "      <td>664573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>VEN</td>\n",
       "      <td>2020</td>\n",
       "      <td>27.951</td>\n",
       "      <td>572356.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>SOM</td>\n",
       "      <td>2008</td>\n",
       "      <td>11.400</td>\n",
       "      <td>150764.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2965</th>\n",
       "      <td>MOZ</td>\n",
       "      <td>2015</td>\n",
       "      <td>27.042</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>ROU</td>\n",
       "      <td>2016</td>\n",
       "      <td>19.761</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>NAM</td>\n",
       "      <td>2011</td>\n",
       "      <td>2.116</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>VEN</td>\n",
       "      <td>2010</td>\n",
       "      <td>28.524</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>ZMB</td>\n",
       "      <td>2008</td>\n",
       "      <td>13.115</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>559 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iso_o  year   pop_o  newarrival  conflict_stock_idp  conflict_idp\n",
       "4768   VEN  2018  28.903   2508056.5                 NaN           NaN\n",
       "4769   VEN  2019  27.817   1524435.0                 NaN           NaN\n",
       "4771   VEN  2021  27.586    664573.0                 NaN           NaN\n",
       "4770   VEN  2020  27.951    572356.0                 NaN           NaN\n",
       "3933   SOM  2008  11.400    150764.0                 NaN           NaN\n",
       "...    ...   ...     ...         ...                 ...           ...\n",
       "2965   MOZ  2015  27.042      1005.0                 NaN           NaN\n",
       "3566   ROU  2016  19.761      1004.0                 NaN           NaN\n",
       "3011   NAM  2011   2.116      1003.0                 NaN           NaN\n",
       "4760   VEN  2010  28.524      1002.0                 NaN           NaN\n",
       "4858   ZMB  2008  13.115      1001.0                 NaN           NaN\n",
       "\n",
       "[559 rows x 6 columns]"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows where conflict data is missing AND the number of refugees is greater than 10,000 AND the year is greater than equal to 2008\n",
    "merged_idp[merged_idp.conflict_idp.isna() & (merged_idp.newarrival > 1000) & (merged_idp.year >= 2008)].sort_values(by='newarrival', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_o</th>\n",
       "      <th>year</th>\n",
       "      <th>pop_o</th>\n",
       "      <th>newarrival</th>\n",
       "      <th>conflict_stock_idp</th>\n",
       "      <th>conflict_idp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2008</td>\n",
       "      <td>22.997</td>\n",
       "      <td>29698.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2009</td>\n",
       "      <td>23.596</td>\n",
       "      <td>41719.0</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2010</td>\n",
       "      <td>24.269</td>\n",
       "      <td>38424.0</td>\n",
       "      <td>352000.0</td>\n",
       "      <td>102000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2011</td>\n",
       "      <td>25.031</td>\n",
       "      <td>45420.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>186000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2012</td>\n",
       "      <td>25.864</td>\n",
       "      <td>60632.0</td>\n",
       "      <td>492000.0</td>\n",
       "      <td>100400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>2016</td>\n",
       "      <td>14.228</td>\n",
       "      <td>9435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>2017</td>\n",
       "      <td>14.437</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>2018</td>\n",
       "      <td>14.642</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>2019</td>\n",
       "      <td>14.905</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>2020</td>\n",
       "      <td>15.189</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1088 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iso_o  year   pop_o  newarrival  conflict_stock_idp  conflict_idp\n",
       "8      AFG  2008  22.997     29698.0                 NaN           NaN\n",
       "9      AFG  2009  23.596     41719.0            297000.0           0.0\n",
       "10     AFG  2010  24.269     38424.0            352000.0      102000.0\n",
       "11     AFG  2011  25.031     45420.0            450000.0      186000.0\n",
       "12     AFG  2012  25.864     60632.0            492000.0      100400.0\n",
       "...    ...   ...     ...         ...                 ...           ...\n",
       "4891   ZWE  2016  14.228      9435.0                 NaN           NaN\n",
       "4892   ZWE  2017  14.437      4523.0                 NaN           NaN\n",
       "4893   ZWE  2018  14.642      1908.0                 NaN           NaN\n",
       "4894   ZWE  2019  14.905      3371.0                 NaN           NaN\n",
       "4895   ZWE  2020  15.189      1196.0                 NaN           NaN\n",
       "\n",
       "[1088 rows x 6 columns]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows where the number of refugees is greater than 10,000 AND the year is greater than equal to 2008\n",
    "merged_idp[(merged_idp.newarrival > 1000) & (merged_idp.year >= 2008) ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that that half of the 1/2 of the outflows where the total number of outflows is greater than 1,000 are missing. This means that missing data isn't simply zero, and it actually is missing data.\n",
    "\n",
    "We'll need to convert this to some sort of categorical variable if we want to use it.\n",
    "\n",
    "Here is the current approach. Where we do $$log(\\frac{conflict\\_variable}{population \\text{ (in 1000s)}} + .0001)$$\n",
    "\n",
    "The .0001 is to ensure that an error isn't produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_idp['conflict_idp_pop'] = merged_idp.apply(lambda row: np.log((row['conflict_idp'] / row['pop_o']) * 1000 + 0.1), axis=1)\n",
    "merged_idp['conflict_stock_idp_pop'] = merged_idp.apply(lambda row: np.log((row['conflict_stock_idp'] / row['pop_o']) * 1000 + 0.1), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do this, we get a distribution where the values are mostly between -6 and 6. The most common value before was zero, which is what the value to the left of -8 is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert to categorical variable, we first convert to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_idp[['conflict_stock_idp_pop', 'conflict_idp_pop']] = merged_idp[['conflict_stock_idp_pop', 'conflict_idp_pop']].apply(lambda x: np.round(x).astype(str))\n",
    "merged_idp.drop(['pop_o','newarrival','conflict_stock_idp','conflict_idp'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do binary encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_o</th>\n",
       "      <th>year</th>\n",
       "      <th>conflict_stock_idp_pop_0</th>\n",
       "      <th>conflict_stock_idp_pop_1</th>\n",
       "      <th>conflict_stock_idp_pop_2</th>\n",
       "      <th>conflict_stock_idp_pop_3</th>\n",
       "      <th>conflict_stock_idp_pop_4</th>\n",
       "      <th>conflict_idp_pop_0</th>\n",
       "      <th>conflict_idp_pop_1</th>\n",
       "      <th>conflict_idp_pop_2</th>\n",
       "      <th>conflict_idp_pop_3</th>\n",
       "      <th>conflict_idp_pop_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>TLS</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>KGZ</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>ARG</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>LVA</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>NPL</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>PAN</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>BLR</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>TGO</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>LCA</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>FRA</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iso_o  year  conflict_stock_idp_pop_0  conflict_stock_idp_pop_1   \n",
       "4361   TLS  2011                         0                         1  \\\n",
       "2346   KGZ  2021                         0                         1   \n",
       "156    ARG  2006                         0                         0   \n",
       "2397   LVA  2022                         0                         0   \n",
       "3061   NPL  2011                         0                         0   \n",
       "3341   PAN  2016                         0                         0   \n",
       "400    BLR  2000                         0                         0   \n",
       "4384   TGO  2009                         0                         1   \n",
       "4079   LCA  2004                         0                         0   \n",
       "1507   FRA  2007                         0                         0   \n",
       "\n",
       "      conflict_stock_idp_pop_2  conflict_stock_idp_pop_3   \n",
       "4361                         1                         0  \\\n",
       "2346                         1                         1   \n",
       "156                          0                         0   \n",
       "2397                         0                         0   \n",
       "3061                         1                         1   \n",
       "3341                         0                         0   \n",
       "400                          0                         0   \n",
       "4384                         1                         0   \n",
       "4079                         0                         0   \n",
       "1507                         0                         0   \n",
       "\n",
       "      conflict_stock_idp_pop_4  conflict_idp_pop_0  conflict_idp_pop_1   \n",
       "4361                         1                   0                   0  \\\n",
       "2346                         1                   0                   0   \n",
       "156                          1                   0                   0   \n",
       "2397                         1                   0                   0   \n",
       "3061                         0                   0                   0   \n",
       "3341                         1                   0                   0   \n",
       "400                          1                   0                   0   \n",
       "4384                         0                   0                   0   \n",
       "4079                         1                   0                   0   \n",
       "1507                         1                   0                   0   \n",
       "\n",
       "      conflict_idp_pop_2  conflict_idp_pop_3  conflict_idp_pop_4  \n",
       "4361                   1                   1                   0  \n",
       "2346                   1                   0                   0  \n",
       "156                    0                   0                   1  \n",
       "2397                   0                   0                   1  \n",
       "3061                   0                   1                   0  \n",
       "3341                   0                   0                   1  \n",
       "400                    0                   0                   1  \n",
       "4384                   0                   1                   0  \n",
       "4079                   0                   0                   1  \n",
       "1507                   0                   0                   1  "
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders.binary import BinaryEncoder\n",
    "be = BinaryEncoder()\n",
    "idp_binary = be.fit_transform(merged_idp[['conflict_stock_idp_pop', 'conflict_idp_pop']])\n",
    "idp_binary = pd.concat([merged_idp.drop(['conflict_stock_idp_pop', 'conflict_idp_pop'], axis=1), idp_binary],axis=1)\n",
    "\n",
    "idp_binary.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 4 columns for each variable, where the values ranging from -8 to 6 or so have been encoded. The missing nan has also been encoded as a unique variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "idp_binary.to_csv('clean/idp.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragile States Index\n",
    "\n",
    "From [here](https://fragilestatesindex.org/)\n",
    "\n",
    "It is yearly from 2006-2022 and contains info on 179 countries. The remaining countries we impute, taking an average of neighbors' fragilitity indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhcr_data = pd.read_csv('clean/unhcr.csv', engine='pyarrow').drop_duplicates(['iso_o','year'])[['iso_o','year','newarrival','Country_o']]\n",
    "unhcr_data = unhcr_data[(unhcr_data.year > 2005) & (unhcr_data.year < 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "unhcr = pd.read_csv('clean/unhcr.csv', engine='pyarrow').groupby(['iso_o','iso_d']).agg({'newarrival':'sum','contig':'first','Country_o':'first','Country_d':'first', 'island_o':'first','dist':'first'}).reset_index()\n",
    "\n",
    "df_network = unhcr[unhcr.contig == 1]\n",
    "\n",
    "graph = ig.Graph.TupleList(df_network[['Country_o','Country_d','dist']].itertuples(index=False), directed=False, edge_attrs='dist')\n",
    "\n",
    "# add island countries \n",
    "islands = unhcr.drop_duplicates('Country_o').sort_values('Country_o').Country_o[~unhcr.groupby('Country_o')['contig'].any().values].values\n",
    "\n",
    "for i in islands:\n",
    "    v = graph.add_vertex()\n",
    "    # Set the name or other properties of the added vertex if needed\n",
    "    v['name'] = i\n",
    "\n",
    "# graph.vs['name'] = coco.convert(graph.vs['name'], to='iso3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel files in\n",
    "import os\n",
    "path = 'raw/fsi 2022-2006/'\n",
    "fsi_files = os.listdir(path)\n",
    "fsi_files = path + pd.Series(fsi_files)\n",
    "fsi_df = pd.concat(fsi_files.apply(lambda x: pd.read_excel(x)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "fsi_df = fsi_df.rename({'Year':'year', 'Total':'fsi_overall', 'C1: Security Apparatus':'fsi_security_apparatus',\n",
    "       'C2: Factionalized Elites':'fsi_factionalized_elites', 'C3: Group Grievance':'fsi_group_grievance', 'E1: Economy':'fsi_economy',\n",
    "       'E2: Economic Inequality':'fsi_economic_inequality', 'E3: Human Flight and Brain Drain':'fsi_human_flight_brain_drain',\n",
    "       'P1: State Legitimacy':'fsi_state_legitimacy', 'P2: Public Services':'fsi_public_services', 'P3: Human Rights':'fsi_human_rights',\n",
    "       'S1: Demographic Pressures':'fsi_demographic_pressures', 'S2: Refugees and IDPs':'fsi_refugees_idps',\n",
    "       'X1: External Intervention':'fsi_external_intervention'}, axis=1).drop(['Change from Previous Year','Rank'],axis=1)\n",
    "\n",
    "# Change an error causing country name. Israel and West Bank -> West Bank and Gaza (consistent with the UNHCR naming convention)\n",
    "fsi_df.loc[fsi_df.Country == 'Israel and West Bank', \"Country\"] = 'West Bank and Gaza'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert country name to iso3\n",
    "import country_converter as coco\n",
    "iso3_unique = coco.convert(names=fsi_df.Country.unique(), to='ISO3')\n",
    "\n",
    "iso_3_dict = dict(zip(fsi_df.Country.unique(), iso3_unique))\n",
    "\n",
    "fsi_df['iso'] = fsi_df['Country'].map(iso_3_dict)\n",
    "\n",
    "fsi_df['year'] = fsi_df['year'].apply(lambda x: x.year if isinstance(x, pd.Timestamp) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those who are missing fragility indices, filter for neighbors and take average of that\n",
    "fsi_full = unhcr_data.merge(fsi_df, how='left',left_on=['iso_o','year'], right_on=['iso','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those who are missing fragility indices, filter for neighbors and take average of that\n",
    "missing_indices = unhcr_data.set_index(['iso_o','year'])[unhcr_data.merge(fsi_df, how='left',left_on=['iso_o','year'], right_on=['iso','year']).set_index(['iso_o','year']).fsi_overall.isna()].reset_index()[['Country_o','year']]\n",
    "\n",
    "for index, row in missing_indices.iterrows():\n",
    "    bordering = graph.vs[graph.neighborhood(row['Country_o'], order=1)]['name'][1:]\n",
    "    \n",
    "    # take the mean of the neighboring countries\n",
    "    if len(bordering) > 0:\n",
    "        fsi_full.loc[(fsi_full.Country_o == row.Country_o) & (fsi_full.year == row.year), fsi_full.columns.str.contains('fsi')] = fsi_full.loc[fsi_full.Country_o.isin(bordering) & (fsi_full.year == row['year']) , fsi_full.columns.str.contains('fsi')].agg('mean').tolist()\n",
    "    # if there are no bordering countries (island), take the 5 closest countries and average. If there are still na's, then take the closest 9 countries.\n",
    "    elif len(unhcr[unhcr.Country_o == row.Country_o].dist.unique()) > 1:\n",
    "        idx = unhcr[unhcr.Country_o == row.Country_o].dist.nsmallest(4).index\n",
    "        closest_countries = unhcr.iloc[idx].Country_d\n",
    "        neighboring_fsi_indices = fsi_full.loc[fsi_full.Country_o.isin(closest_countries) & (fsi_full.year == row.year), fsi_full.columns.str.contains('fsi')].agg('mean')\n",
    "        if neighboring_fsi_indices.isna().sum() == 0:\n",
    "            fsi_full.loc[(fsi_full.Country_o == row.Country_o) & (fsi_full.year == row.year), fsi_full.columns.str.contains('fsi')] = neighboring_fsi_indices.tolist()\n",
    "        else:\n",
    "            idx = unhcr[unhcr.Country_o == row.Country_o].dist.nsmallest(9).index\n",
    "            closest_countries = unhcr.iloc[idx].Country_d\n",
    "            neighboring_fsi_indices = fsi_full.loc[fsi_full.Country_o.isin(closest_countries) & (fsi_full.year == row.year), fsi_full.columns.str.contains('fsi')].agg('mean')\n",
    "            fsi_full.loc[(fsi_full.Country_o == row.Country_o) & (fsi_full.year == row.year), fsi_full.columns.str.contains('fsi')] = neighboring_fsi_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsi_full.drop(['Country','Country_o','newarrival','iso'], axis=1).rename({'iso_o':'iso'},axis=1).to_csv('clean/fragility_indices.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modeled Newspaper Text/Conflict Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# we read in the data, remove nas, and drop the first column for each forecast. \n",
    "ons_armedconf1 = pd.read_csv('raw/ons_armedconf12_f_12_d_50_t_700_depth_7_mins_2_minl_50pctrial_low_pop_og.csv')\n",
    "ons_armedconf1 = ons_armedconf1.loc[~ons_armedconf1.isna().any(axis=1)].iloc[:, 1:ons_armedconf1.shape[1]]\n",
    "\n",
    "ons_armedconf2 = pd.read_csv('raw/ons_armedconf12_f_12_d_50_t_700_depth_7_mins_2_minl_100pctrial_low_pop_og.csv')\n",
    "ons_armedconf2 = ons_armedconf2.loc[~ons_armedconf2.isna().any(axis=1)].iloc[:, 1:ons_armedconf2.shape[1]]\n",
    "\n",
    "ons_armedconf3 = pd.read_csv('raw/lnbest_pc_12_f_12_d_50_t_700_depth_7_mins_2_minl_75intensity_pc_og.csv')\n",
    "ons_armedconf3 = ons_armedconf3.loc[~ons_armedconf3.isna().any(axis=1)].iloc[:, 1:ons_armedconf3.shape[1]]\n",
    "\n",
    "#lda data\n",
    "lda = pd.read_csv('raw/LDA output Mueller and Rauh.csv')\n",
    "lda = lda[lda.year >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the conflict forecasts together\n",
    "merged_armed_conf = ons_armedconf1.merge(ons_armedconf2, on = ['isocode','year', 'month']).merge(ons_armedconf3, on = ['isocode','year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing data \n",
    "unhcr_data = pd.read_csv('clean/unhcr.csv', engine='pyarrow').drop_duplicates(['iso_o','year'])[['iso_o','year','Country_o']]\n",
    "unhcr_data = unhcr_data[ (unhcr_data.year < 2023)]\n",
    "\n",
    "lda_full = unhcr_data.merge(lda, left_on=['iso_o','year'], right_on=['isocode','year'],how='left').drop('isocode',axis=1).rename({'iso_o':'isocode'},axis=1)\n",
    "conflict_full = unhcr_data.merge(merged_armed_conf, left_on=['iso_o','year'], right_on=['isocode','year'],how='left').drop('isocode',axis=1).rename({'iso_o':'isocode'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_resampling(dataframe):\n",
    "    # Create a new DataFrame with a complete range of dates and reindex the original DataFrame\n",
    "    min_date = pd.to_datetime('2000-01-01')\n",
    "    max_date = pd.to_datetime('2023-01-01')\n",
    "    reindexed_df = dataframe.reindex(pd.date_range(start=min_date, end=max_date, freq='MS'))\n",
    "\n",
    "    return reindexed_df\n",
    "\n",
    "def resample_missing_dates(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    dataframe.loc[dataframe.month.isna(), 'month'] = 1\n",
    "\n",
    "    # Combine 'year' and 'month' columns into a single column\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['year'].astype(str) + '-' + dataframe['month'].astype(int).astype(str), format='%Y-%m')\n",
    "    \n",
    "    # Set the 'date' column as the index\n",
    "    dataframe.set_index('date', inplace=True)\n",
    "    \n",
    "    # Resample the dataframe to fill missing year-month values for each country\n",
    "    resampled_dataframe = dataframe.groupby(['Country_o','isocode'], as_index=False).apply(prepare_data_for_resampling)\n",
    "    resampled_dataframe = resampled_dataframe.droplevel(0).groupby(['Country_o','isocode']).apply(lambda x: x.resample('MS').asfreq())\n",
    "\n",
    "    # Reset the index to get back the 'date' column\n",
    "    resampled_dataframe = resampled_dataframe.drop(['Country_o','year','month','isocode'], axis=1).reset_index().rename({'level_2':'date'},axis=1)\n",
    "\n",
    "    # Split the 'date' column back into 'year' and 'month'\n",
    "    resampled_dataframe['year'] = resampled_dataframe['date'].dt.year\n",
    "    resampled_dataframe['month'] = resampled_dataframe['date'].dt.month\n",
    "    resampled_dataframe.drop('date', axis=1, inplace=True)\n",
    "    \n",
    "    return resampled_dataframe\n",
    "\n",
    "lda_full = resample_missing_dates(lda_full)\n",
    "conflict_full = resample_missing_dates(conflict_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cols = ['tokens', 'obs',\n",
    "       'sentiment_words', 'uncertainty', 'ste_theta0', 'ste_theta1',\n",
    "       'ste_theta2', 'ste_theta3', 'ste_theta4', 'ste_theta5', 'ste_theta6',\n",
    "       'ste_theta7', 'ste_theta8', 'ste_theta9', 'ste_theta10', 'ste_theta11',\n",
    "       'ste_theta12', 'ste_theta13', 'ste_theta14', 'ste_theta0_stock',\n",
    "       'ste_theta1_stock', 'ste_theta2_stock', 'ste_theta3_stock',\n",
    "       'ste_theta4_stock', 'ste_theta5_stock', 'ste_theta6_stock',\n",
    "       'ste_theta7_stock', 'ste_theta8_stock', 'ste_theta9_stock',\n",
    "       'ste_theta10_stock', 'ste_theta11_stock', 'ste_theta12_stock',\n",
    "       'ste_theta13_stock', 'ste_theta14_stock', 'sentiment_stock',\n",
    "       'uncertainty_index', 'best']\n",
    "conflict_cols = ['ons_armedconf12_pred_jut_x',\n",
    "       'ons_armedconf12_pred_top_x', 'ons_armedconf12_pred_aug_x',\n",
    "       'ons_armedconf12_pred_jut_y', 'ons_armedconf12_pred_top_y',\n",
    "       'ons_armedconf12_pred_aug_y', 'lnbest_pc_12_pred_jut',\n",
    "       'lnbest_pc_12_pred_top', 'lnbest_pc_12_pred_aug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate mean of neighboring countries\n",
    "def calculate_mean_neighbors(dataframe, row, bordering, cols):\n",
    "    if len(bordering) > 0:\n",
    "        return dataframe.loc[dataframe.Country_o.isin(bordering) & (dataframe.year == row['year']) & (dataframe.month == row['month']), cols].agg('mean').tolist()\n",
    "    else:\n",
    "        idx = unhcr[unhcr.Country_o == row.Country_o].dist.nsmallest(4).index\n",
    "        closest_countries = unhcr.iloc[idx].Country_d\n",
    "        neighboring_indices = dataframe.loc[dataframe.Country_o.isin(closest_countries) & (dataframe.year == row.year) & (dataframe.month == row.month), cols].agg('mean')\n",
    "        if neighboring_indices.isna().sum() == 0:\n",
    "            return neighboring_indices.tolist()\n",
    "        else:\n",
    "            idx = unhcr[unhcr.Country_o == row.Country_o].dist.nsmallest(9).index\n",
    "            closest_countries = unhcr.iloc[idx].Country_d\n",
    "            neighboring_indices = dataframe.loc[dataframe.Country_o.isin(closest_countries) & (dataframe.year == row.year) & (dataframe.month == row.month), cols].agg('mean')\n",
    "            return neighboring_indices.tolist()\n",
    "\n",
    "def is_match(df, row):\n",
    "    return (df.Country_o == row.Country_o) & (df.year == row.year) & (df.month == row.month)\n",
    "\n",
    "# Process missing fragility indices in lda_full dataframe\n",
    "missing_indices_lda = lda_full[lda_full.ste_theta0.isna()][['Country_o','year','month']]\n",
    "\n",
    "\n",
    "for index, row in missing_indices_lda.iterrows():\n",
    "    bordering = graph.vs[graph.neighborhood(row['Country_o'], order=1)]['name'][1:]\n",
    "    lda_full.loc[is_match(lda_full, row), lda_cols] = calculate_mean_neighbors(lda_full, row, bordering, lda_cols)\n",
    "\n",
    "# Process missing fragility indices in conflict_full dataframe\n",
    "missing_indices_conflict = conflict_full[conflict_full.ons_armedconf12_pred_top_y.isna()][['Country_o','year','month']]\n",
    "\n",
    "for index, row in missing_indices_conflict.iterrows():\n",
    "    bordering = graph.vs[graph.neighborhood(row['Country_o'], order=1)]['name'][1:]\n",
    "    conflict_full.loc[is_match(conflict_full, row), conflict_cols] = calculate_mean_neighbors(conflict_full, row, bordering, conflict_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "conflict_full.to_csv('clean/mueller_conflict_forecasts.csv')\n",
    "lda_full.to_csv('clean/lda.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gdelt = pd.read_csv('raw/final_gdelt_bycountry.txt', engine='pyarrow')\n",
    "\n",
    "# replace XKX code with  UNHCR code for Kosovo\n",
    "gdelt.loc[gdelt.isocode == 'XKX', 'isocode'] = 'UVK'\n",
    "\n",
    "# add WBG (just assume the same freqeuency of events as Israel)\n",
    "WBG = gdelt.reset_index().copy()\n",
    "WBG = WBG[WBG.isocode.isin(['ISR'])].copy().reset_index(drop=True)\n",
    "WBG.loc[:, 'isocode'] = 'WBG'\n",
    "\n",
    "gdelt = pd.concat([gdelt, WBG], axis=0)\n",
    "\n",
    "# aggregate goldstein, taking avg goldstein score by month\n",
    "goldstein = gdelt[gdelt.year >=2000].set_index(['year','month','isocode'])['AvgGoldsteinScale'].groupby(['year','month','isocode']).agg(lambda x: x.fillna(x.mean()).mean())\n",
    "\n",
    "# if no goldstein that month, fill by year\n",
    "goldstein = goldstein.groupby(['year','isocode'], group_keys=False).apply(lambda x: x.fillna(x.mean()))\n",
    "# if none in the year, impute the mean\n",
    "goldstein = goldstein.fillna(goldstein.mean())\n",
    "\n",
    "gdelt = gdelt.loc[gdelt.year >=2000, ~gdelt.columns.str.contains('ADM1Code|Actor|gov|opp|Goldstein')].groupby(['year','month','isocode']).agg(sum)\n",
    "\n",
    "# to account for the variance in recording of events across time, divide by the total number of events for that country/year\n",
    "gdelt = gdelt.div(gdelt.sum(axis=1), axis=0)\n",
    "gdelt = pd.concat([gdelt, goldstein], axis=1)\n",
    "\n",
    "gdelt.to_csv('clean/gdelt.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refugees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
