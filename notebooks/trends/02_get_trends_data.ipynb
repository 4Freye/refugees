{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Trends Data\n",
    "This notebook requests trends data on topics, namely relevant terms like destination cities and destination countries. The topic ids have already been collected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions below is how we make reqeusts to google trends to return trends on keywords.\n",
    "\n",
    "It is a bit of trial and error and a bit of help from [this post](https://stackoverflow.com/a/67199394/10006534).\n",
    "\n",
    "It gathers one term at a time from a list of trends and then if an error occurs (which often happens due to the fact that Google Trends is rate-limited), it sleeps for a minute and repeats the request. If 20 requests are made in a row that result in an error, it will skip that particular request and move on to the next term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of language codes from googletrans\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/112.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    # 'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Referer': 'https://trends.google.com/',\n",
    "    'Alt-Used': 'trends.google.com',\n",
    "    'Connection': 'keep-alive',\n",
    "    # 'Cookie': '__utma=10102256.699944976.1681467038.1683327769.1683363479.30; __utmz=10102256.1683363479.30.23.utmcsr=trends.google.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utmc=10102256; NID=511=GaXIe0Lwd1l8RAGkA2geWNynqviDUhjPBcVgHksJdTnugCvKuUPbm_bM-mT7DhT2jrBHT00aCt71oY7fZhydICB-HNWUzrDnonyPyOGmPTA75lOvpTiguXi3KiGJtRjK3BBH3e1ZcqQ_ywcsU5vHoxJFtH9HGhcLdOt7CL7AWKx8Jj9VSOI3cCwmjDl8gbj2PZ75BU_W4NqspBRMktcdhRitXCyOIqMdLMwZfSOOvFmRBTOJKg8M7UkUTwAVhXtxsKVlHfxPpiWx8HQ63Vr5SV_8qW9f4J0f8EbXWiofQLqpPKJzo0CMbyM-EcnRlR4YVqptEli6EgemOBUJAgH8951i7ANgVDSWy-vn3zXA5KPR5l0LtkriirFZPvsNAmV-_-Mtyuf6gYu8eYJL3g; CONSENT=PENDING+639; SID=WAjkbwUHGFuugy4Yy2rq46Op5ZjRIMvPaLQIAltzHSM35MU0x7YgYongisCrn5htv3RhAw.; __Secure-1PSID=WAjkbwUHGFuugy4Yy2rq46Op5ZjRIMvPaLQIAltzHSM35MU0YH1nUovIFt-jaEUUCx_SIQ.; __Secure-3PSID=WAjkbwUHGFuugy4Yy2rq46Op5ZjRIMvPaLQIAltzHSM35MU0Z8qXUg1LhjB4DtBZWFfNQg.; HSID=A8QJObb1Ve4vQOXFw; SSID=AJr3GRs7Jf_ctBT41; APISID=rNTBsHwZF0AVrKao/AoTWce3Qv8CyFykEc; SAPISID=vftmcyrgIFqWdYpV/AHlhj91rgxiQPlOq8; __Secure-1PAPISID=vftmcyrgIFqWdYpV/AHlhj91rgxiQPlOq8; __Secure-3PAPISID=vftmcyrgIFqWdYpV/AHlhj91rgxiQPlOq8; SIDCC=AP8dLtyjVDmjXvg3rEmTwoLfGyXkY0SDrIFQWqi1z9D1QOL5voioH1Uti_ANGJkiQCuzVd4Axww; __Secure-1PSIDCC=AP8dLtxxVvSKM2MgLGepw_20VZbYsJHar-zF5kvDajRKezVqui3YqxWUaT1e6meVcR9HTUP4lgo; __Secure-3PSIDCC=AP8dLtyyI8BLnakxZZ2OFmPTDfYzPW8jo13jnE34rpPuptgnFDFq-aKX5vfcZdtRDLLZswyAl3gv; 1P_JAR=2023-5-6-12; SOCS=CAISHAgCEhJnd3NfMjAyMjEwMDQtMF9SQzMaAmVuIAEaBgiAwY2aBg; AEC=AUEFqZchyarTzQblW5K5GOTGtYARrs8luJGdx84JVmSwETHSFqijMgs9FA; _ga_VWZPXDNJJB=GS1.1.1683458025.38.1.1683458061.0.0.0; _ga=GA1.3.699944976.1681467038; OTZ=6986051_48_52_123900_48_436380; ADS_VISITOR_ID=00000000-0000-0000-0000-000000000000/112727363205027642159; S=billing-ui-v3=wWfIrmncuOn4LfU6DArDU3LLPpCDgsAT:billing-ui-v3-efe=wWfIrmncuOn4LfU6DArDU3LLPpCDgsAT; __Secure-1PSIDTS=sidts-CjIBLFra0jgJEQyM4EqRZoyaN18X_Umt8M6GTvixMw1pDB_sj5P5XvQokN5dkVw1R2qAkRAA; __Secure-3PSIDTS=sidts-CjIBLFra0jgJEQyM4EqRZoyaN18X_Umt8M6GTvixMw1pDB_sj5P5XvQokN5dkVw1R2qAkRAA; _gid=GA1.3.1220682113.1683458025; _gat_gtag_UA_4401283=1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    # Requests doesn't support trailers\n",
    "    # 'TE': 'trailers',\n",
    "}\n",
    "\n",
    "def pytrends_request(word_list, country, pytrends, start_date, end_date):\n",
    "    \n",
    "    pytrends.build_payload(kw_list=word_list, geo=country, timeframe=start_date + ' ' + end_date)\n",
    "    trends = pytrends.interest_over_time()\n",
    "    if 'isPartial' in trends.columns:\n",
    "        trends.drop('isPartial', axis=1, inplace=True)\n",
    "    # print(word_list)\n",
    "    return trends\n",
    "\n",
    "def get_trends_data(country, keywords, start_date, end_date):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, requests_args={'headers': headers})\n",
    "    trends_df = pd.DataFrame()\n",
    "    error_count = 0\n",
    "\n",
    "    for keyword in keywords:\n",
    "        while True:\n",
    "            try:\n",
    "                trends_df = pd.concat([trends_df, pytrends_request([keyword], country, pytrends, start_date, end_date)], axis=1)\n",
    "                error_count = 0  # Reset error count if successful request\n",
    "                break  # Exit the while loop if successful\n",
    "            except:\n",
    "                error_count += 1\n",
    "                # print('Got an error. Trying again in 60 seconds.')\n",
    "                time.sleep(60)\n",
    "\n",
    "                if error_count == 20:\n",
    "                    print('Reached maximum error count. Exiting loop.')\n",
    "                    return trends_df  # Return the trends_df even if not complete\n",
    "    return trends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2005-01-01'\n",
    "end_date = '2023-01-01'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Link Topic Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_topic_ids = pd.read_csv('topic_ids/semantic_topic_ids.csv')\n",
    "countries = pd.read_csv('../../data/clean/unhcr.csv', engine='pyarrow').drop_duplicates('iso_o').Country_o\n",
    "import country_converter as coco\n",
    "iso2_countries = coco.convert(countries, to='iso2')\n",
    "\n",
    "semantic_dict = semantic_topic_ids[['keyword','topic_id']].set_index('topic_id')['keyword'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [15:43:42<00:00, 288.89s/it]    \n"
     ]
    }
   ],
   "source": [
    "country_trends_list = []\n",
    "for iso2country in tqdm(iso2_countries):\n",
    "    a_country_trends = get_trends_data(iso2country, semantic_topic_ids.topic_id, '2005-01-01', '2023-01-01')\n",
    "    a_country_trends['country'] = iso2country\n",
    "    country_trends_list.append(a_country_trends)\n",
    "\n",
    "semantic_trends_df = pd.DataFrame()\n",
    "for idx, a_country_semantic_trends in enumerate(country_trends_list):\n",
    "    a_country = a_country_semantic_trends.copy()\n",
    "    if a_country.index.name == 'date':\n",
    "        a_country.reset_index(inplace=True)\n",
    "    if 'index' in a_country.columns.values:\n",
    "        a_country.drop('index',axis=1, inplace=True)\n",
    "    a_country = a_country.loc[:, ~a_country.columns.duplicated()]\n",
    "    # a_country.set_index(['date','country'], inplace=True)\n",
    "    a_country.rename(columns=semantic_dict, inplace=True)\n",
    "    semantic_trends_df = pd.concat([semantic_trends_df, a_country], axis=0, ignore_index=True)\n",
    "\n",
    "semantic_trends_df.to_csv('data/semantic_topic_trends_2005_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_trends_df[semantic_trends_df[['passport', 'Immigration', 'Travel Visa', 'Refugee', 'Conflict',\n",
    "       'Violence', 'Crisis', 'Militia', 'Genocide', 'Armed Forces', 'Civilian',\n",
    "       'Currency', 'Lottery', 'Economy', 'Bureau de change', 'Wage', 'Protest',\n",
    "       'Coup d’état', 'Government']].notna().any(axis=1)].country.unique().__len__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighboring Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import country_converter as coco\n",
    "\n",
    "# convert unhcr data to network format. To produce the unhcr.csv file, you will need to:\n",
    "# # drag and drop the data.csv file from geraldine into the data/raw/ folder\n",
    "# # open the clean_data.ipynb notebook in data/\n",
    "# # run the section that cleans the unhcr data, which outputs unhcr.csv into data/clean/\n",
    "unhcr = pd.read_csv('../../data/clean/unhcr.csv', engine='pyarrow').groupby(['iso_o','iso_d']).agg({'newarrival':'sum','contig':'first','Country_o':'first','Country_d':'first', 'island_o':'first', 'dist':'first'}).reset_index()\n",
    "\n",
    "df_network = unhcr[unhcr.contig == 1]\n",
    "\n",
    "graph = ig.Graph.TupleList(df_network[['Country_o','Country_d']].itertuples(index=False), directed=False)\n",
    "\n",
    "# add island countries \n",
    "islands = unhcr.drop_duplicates('Country_o').sort_values('Country_o').Country_o[~unhcr.groupby('Country_o')['contig'].any().values].values\n",
    "\n",
    "for i in islands:\n",
    "    v = graph.add_vertex()\n",
    "    # Set the name or other properties of the added vertex if needed\n",
    "    v['name'] = i\n",
    "\n",
    "graph.vs['name'] = coco.convert(graph.vs['name'], to='iso2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get country topic ids\n",
    "country_topic_ids = pd.read_csv('topic_ids/country_topic_ids.csv')\n",
    "country_topic_ids['iso2'] = coco.convert(country_topic_ids.search, to='iso2')\n",
    "country_topic_dict = country_topic_ids[['topic_title', 'topic_mid']].set_index('topic_mid')['topic_title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [4:20:29<00:00, 79.74s/it]   \n"
     ]
    }
   ],
   "source": [
    "# list of countries\n",
    "iso2_countries = coco.convert(unhcr.Country_o.unique(), to='iso2')\n",
    "\n",
    "country_trends_df = pd.DataFrame()\n",
    "country_trends_list = []\n",
    "for iso2country in tqdm(iso2_countries):\n",
    "    # get neighbors of country\n",
    "    neighboring_countries = graph.vs[graph.neighborhood(iso2country, order=1)]['name'][1:]\n",
    "\n",
    "    # if no bordering countries (islands), take the 3 closest countries\n",
    "    if len(neighboring_countries) == 0:\n",
    "        neighboring_countries = coco.convert(unhcr[unhcr.iso_o  == coco.convert(iso2country, to='iso3')].nsmallest(3, 'dist').iso_d.iloc[0:3], to='iso2')\n",
    "    \n",
    "    order1_countries = country_topic_ids[country_topic_ids.iso2.isin(neighboring_countries)]\n",
    "\n",
    "    a_country_trends = get_trends_data(iso2country, order1_countries.topic_mid, start_date, end_date)\n",
    "    \n",
    "    if len(a_country_trends) != 0:\n",
    "        a_country_trends = a_country_trends.reset_index().melt(id_vars='date', var_name = 'topic_mid')\n",
    "        a_country_trends['country_o'] = iso2country\n",
    "        country_trends_df = pd.concat([country_trends_df, a_country_trends], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_trends_df.to_csv('data/2005_topics/country_topic_trends_2005_complete.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can gather trends for countries of order 2, excluding order 1 \n",
    "\n",
    "(I think we can skp this part for now. There needs to be some distance-based filter as well that omits far away countries, Looking at Afghanistan for example yields too many countries/cities).\n",
    "\n",
    "This could be obtained by merging countries with the unhcr distance measurments between countries, and omitting countries above a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search</th>\n",
       "      <th>topic_title</th>\n",
       "      <th>topic_type</th>\n",
       "      <th>topic_mid</th>\n",
       "      <th>iso2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>Country in Asia</td>\n",
       "      <td>/m/0jgx</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>Country</td>\n",
       "      <td>/m/0jhd</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bhutan</td>\n",
       "      <td>Bhutan</td>\n",
       "      <td>Country in South Asia</td>\n",
       "      <td>/m/07bxhl</td>\n",
       "      <td>BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Hong Kong SAR</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Special administrative regions of China</td>\n",
       "      <td>/m/03h64</td>\n",
       "      <td>HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>India</td>\n",
       "      <td>India</td>\n",
       "      <td>Country in South Asia</td>\n",
       "      <td>/m/03rk0</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Iraq</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>Country in the Middle East</td>\n",
       "      <td>/m/0d05q4</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>Country in Central Asia</td>\n",
       "      <td>/m/047lj</td>\n",
       "      <td>KZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Kyrgyz Republic</td>\n",
       "      <td>Kyrgyzstan</td>\n",
       "      <td>Country in Central Asia</td>\n",
       "      <td>/m/0jt3tjf</td>\n",
       "      <td>KG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Lao P.D.R.</td>\n",
       "      <td>Laos</td>\n",
       "      <td>Country in Asia</td>\n",
       "      <td>/m/04hhv</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Macao SAR</td>\n",
       "      <td>Macao</td>\n",
       "      <td>Special administrative regions of China</td>\n",
       "      <td>/m/04thp</td>\n",
       "      <td>MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Mongolia</td>\n",
       "      <td>Mongolia</td>\n",
       "      <td>Country in East Asia</td>\n",
       "      <td>/m/04w8f</td>\n",
       "      <td>MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Myanmar</td>\n",
       "      <td>Myanmar (Burma)</td>\n",
       "      <td>Country in Asia</td>\n",
       "      <td>/m/04xn_</td>\n",
       "      <td>MM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>Country in South Asia</td>\n",
       "      <td>/m/016zwt</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Country</td>\n",
       "      <td>/m/06bnz</td>\n",
       "      <td>RU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Türkiye</td>\n",
       "      <td>Türkiye</td>\n",
       "      <td>Country in the Middle East</td>\n",
       "      <td>/m/01znc_</td>\n",
       "      <td>TR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Country in Asia</td>\n",
       "      <td>/m/01crd5</td>\n",
       "      <td>VN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              search      topic_title  \\\n",
       "6            Armenia          Armenia   \n",
       "10        Azerbaijan       Azerbaijan   \n",
       "19            Bhutan           Bhutan   \n",
       "73     Hong Kong SAR        Hong Kong   \n",
       "76             India            India   \n",
       "79              Iraq             Iraq   \n",
       "86        Kazakhstan       Kazakhstan   \n",
       "92   Kyrgyz Republic       Kyrgyzstan   \n",
       "93        Lao P.D.R.             Laos   \n",
       "101        Macao SAR            Macao   \n",
       "114         Mongolia         Mongolia   \n",
       "118          Myanmar  Myanmar (Burma)   \n",
       "121            Nepal            Nepal   \n",
       "142           Russia           Russia   \n",
       "178          Türkiye          Türkiye   \n",
       "190          Vietnam          Vietnam   \n",
       "\n",
       "                                  topic_type   topic_mid iso2  \n",
       "6                            Country in Asia     /m/0jgx   AM  \n",
       "10                                   Country     /m/0jhd   AZ  \n",
       "19                     Country in South Asia   /m/07bxhl   BT  \n",
       "73   Special administrative regions of China    /m/03h64   HK  \n",
       "76                     Country in South Asia    /m/03rk0   IN  \n",
       "79                Country in the Middle East   /m/0d05q4   IQ  \n",
       "86                   Country in Central Asia    /m/047lj   KZ  \n",
       "92                   Country in Central Asia  /m/0jt3tjf   KG  \n",
       "93                           Country in Asia    /m/04hhv   LA  \n",
       "101  Special administrative regions of China    /m/04thp   MO  \n",
       "114                     Country in East Asia    /m/04w8f   MN  \n",
       "118                          Country in Asia    /m/04xn_   MM  \n",
       "121                    Country in South Asia   /m/016zwt   NP  \n",
       "142                                  Country    /m/06bnz   RU  \n",
       "178               Country in the Middle East   /m/01znc_   TR  \n",
       "190                          Country in Asia   /m/01crd5   VN  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighboring_countries_order2 = list(set(graph.vs[graph.neighborhood('AF', order=2)]['name']) - set(graph.vs[graph.neighborhood('AF', order=1)]['name']))\n",
    "\n",
    "# too many countries for order 2.\n",
    "country_topic_ids[country_topic_ids.iso2.isin(neighboring_countries_order2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [22:56:49<00:00, 421.48s/it]   \n"
     ]
    }
   ],
   "source": [
    "# list of countries\n",
    "iso2_countries = coco.convert(unhcr.Country_o.unique(), to='iso2')\n",
    "\n",
    "country_trends_df = pd.DataFrame()\n",
    "country_trends_list = []\n",
    "for iso2country in tqdm(iso2_countries):\n",
    "    # get neighbors of country\n",
    "    neighboring_countries_order2 = list(set(graph.vs[graph.neighborhood(iso2country, order=2)]['name']) - set(graph.vs[graph.neighborhood(iso2country, order=1)]['name']))\n",
    "\n",
    "    # if no bordering countries (islands), take the next 3 closest countries after the 3 closest.\n",
    "    if len(neighboring_countries_order2) == 0:\n",
    "        neighboring_countries_order2 = coco.convert(unhcr[unhcr.iso_o  == coco.convert(iso2country, to='iso3')].nsmallest(3, 'dist').iso_d.iloc[3:6], to='iso2')\n",
    "    \n",
    "    order2_countries = country_topic_ids[country_topic_ids.iso2.isin(neighboring_countries_order2)]\n",
    "\n",
    "    a_country_trends = get_trends_data(iso2country, order2_countries.topic_mid, start_date, end_date)\n",
    "    \n",
    "    if len(a_country_trends) != 0:\n",
    "        a_country_trends = a_country_trends.reset_index().melt(id_vars='date', var_name = 'topic_mid')\n",
    "        a_country_trends['country_o'] = iso2country\n",
    "        country_trends_df = pd.concat([country_trends_df, a_country_trends], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_trends_df.to_csv('data/2005_topics/country_topic_trends_2005_order2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining Countries\n",
    "\n",
    "I calculate the degrees of separation for all the possible pairs.\n",
    "\n",
    "Then I take the top 500 pairs that aren't order 1 or order 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_of_sep = pd.read_csv('degrees_of_separation.csv')\n",
    "unhcr = unhcr.merge(deg_of_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_remaining_pairs = unhcr[~unhcr.degrees_of_separation.isin([1,2])].sort_values(by='newarrival',ascending=False).head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_iso2_dict = dict(zip(unhcr.Country_o.unique(), coco.convert(unhcr.Country_o.unique(), to='iso2')))\n",
    "top_remaining_pairs['iso2_o'] = top_remaining_pairs.Country_o.map(country_iso2_dict)\n",
    "top_remaining_pairs['iso2_d'] = top_remaining_pairs.Country_d.map(country_iso2_dict)\n",
    "\n",
    "remaining_iso2 = top_remaining_pairs.drop_duplicates('iso2_o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [5:51:24<00:00, 236.90s/it]   \n"
     ]
    }
   ],
   "source": [
    "country_trends_df = pd.DataFrame()\n",
    "country_trends_list = []\n",
    "for iso2country in tqdm(remaining_iso2.iso2_o):\n",
    "    top_d_countries = top_remaining_pairs[top_remaining_pairs.iso2_o == iso2country].iso2_d\n",
    "    top_d_countries_topic_ids = country_topic_ids[country_topic_ids.iso2.isin(top_d_countries)].topic_mid   \n",
    "\n",
    "    a_country_trends = get_trends_data(iso2country, top_d_countries_topic_ids, start_date, end_date)\n",
    "    \n",
    "    if len(a_country_trends) != 0:\n",
    "        a_country_trends = a_country_trends.reset_index().melt(id_vars='date', var_name = 'topic_mid')\n",
    "        a_country_trends['country_o'] = iso2country\n",
    "        country_trends_df = pd.concat([country_trends_df, a_country_trends], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_trends_df.to_csv('data/2005_topics/remaining_500_country_trends.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighboring Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get city topic ids\n",
    "city_topic_ids = pd.read_csv('topic_ids/city_topic_id.csv')\n",
    "city_topic_ids['iso2'] = coco.convert(city_topic_ids.search_country, to='iso2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can gather all the cities of neighboring countries of order 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of countries\n",
    "iso2_countries = coco.convert(unhcr.Country_o.unique(), to='iso2')\n",
    "\n",
    "city_trends_list = []\n",
    "city_trends_df = pd.DataFrame()\n",
    "for iso2country in tqdm(iso2_countries):\n",
    "    # get neighbors of country\n",
    "    neighboring_countries = graph.vs[graph.neighborhood(iso2country, order=1)]['name'][1:]\n",
    "\n",
    "     # if no bordering countries (islands), take the 3 closest countries\n",
    "    if len(neighboring_countries) == 0:\n",
    "        neighboring_countries = coco.convert(unhcr[unhcr.iso_o  == coco.convert(iso2country, to='iso3')].nsmallest(3, 'dist').iso_d.iloc[0:3], to='iso2')\n",
    "\n",
    "    order1_cities = city_topic_ids[city_topic_ids.iso2.isin(neighboring_countries)]\n",
    "    a_country_trends = get_trends_data(iso2country, order1_cities.topic_mid, start_date, end_date)\n",
    "    a_country_trends['country_o'] = iso2country\n",
    "    \n",
    "    city_trends_list.append(a_country_trends)\n",
    "\n",
    "    if len(a_country_trends) != 0:\n",
    "        a_country_trends = a_country_trends.reset_index().melt(id_vars='date', var_name = 'topic_mid')\n",
    "        a_country_trends['country_o'] = iso2country\n",
    "        city_trends_df = pd.concat([city_trends_df, a_country_trends], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_trends_df.to_csv('data/2005_topics/cities_topic_trends_2005_order1.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can gather trends for countries of order 2, excluding order 1 \n",
    "\n",
    "(I think we can skp this part for now. There needs to be some distance-based filter as well that omits far away countries, Looking at Afghanistan for example yields too many countries/cities).\n",
    "\n",
    "this could be obtained by using the location coordinates from geonames-all-cities-with-a-population-1000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [17:54:31<00:00, 328.94s/it]   \n"
     ]
    }
   ],
   "source": [
    "# list of countries\n",
    "iso2_countries = coco.convert(unhcr.Country_o.unique(), to='iso2')\n",
    "\n",
    "city_trends_df = pd.DataFrame()\n",
    "country_trends_list = []\n",
    "for iso2country in tqdm(iso2_countries):\n",
    "    # get neighbors of country\n",
    "    neighboring_countries_order2 = list(set(graph.vs[graph.neighborhood(iso2country, order=2)]['name']) - set(graph.vs[graph.neighborhood(iso2country, order=1)]['name']))\n",
    "\n",
    "    # if no bordering countries (islands), take the next 3 closest countries after the 3 closest.\n",
    "    if len(neighboring_countries_order2) == 0:\n",
    "        neighboring_countries_order2 = coco.convert(unhcr[unhcr.iso_o  == coco.convert(iso2country, to='iso3')].nsmallest(6, 'dist').iso_d.iloc[3:6], to='iso2')\n",
    "    \n",
    "    order2_cities = city_topic_ids[city_topic_ids.iso2.isin(neighboring_countries_order2)]\n",
    "\n",
    "    a_country_trends = get_trends_data(iso2country, order2_cities.topic_mid, start_date, end_date)\n",
    "    \n",
    "    if len(a_country_trends) != 0:\n",
    "        a_country_trends = a_country_trends.reset_index().melt(id_vars='date', var_name = 'topic_mid')\n",
    "        a_country_trends['country_o'] = iso2country\n",
    "        city_trends_df = pd.concat([city_trends_df, a_country_trends], axis=0)\n",
    "\n",
    "city_trends_df.to_csv('data/2005_topics/cities_topic_trends_2005_order2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top remaining countries' destination cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [18:09<00:00, 12.25s/it] \n"
     ]
    }
   ],
   "source": [
    "city_trends_df = pd.DataFrame()\n",
    "city_trends_list = []\n",
    "for iso2country in tqdm(remaining_iso2.iso2_o):\n",
    "    top_d_countries = top_remaining_pairs[top_remaining_pairs.iso2_o == iso2country].iso2_d\n",
    "    top_d_countries_topic_ids = city_topic_ids[city_topic_ids.iso2.isin(top_d_countries)].topic_mid   \n",
    "\n",
    "    a_country_trends = get_trends_data(iso2country, top_d_countries_topic_ids, start_date, end_date)\n",
    "    \n",
    "    if len(a_country_trends) != 0:\n",
    "        a_country_trends = a_country_trends.reset_index().melt(id_vars='date', var_name = 'topic_mid')\n",
    "        a_country_trends['country_o'] = iso2country\n",
    "        city_trends_df = pd.concat([city_trends_df, a_country_trends], axis=0)\n",
    "\n",
    "country_trends_df.to_csv('data/2005_topics/cities_topic_trends_2005_top_remaining.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighboring Border Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import country_converter as coco\n",
    "\n",
    "neighboring_city_ids = pd.read_csv('topic_ids/neighboring_city_topic_id.csv')\n",
    "neighboring_city_ids['iso2'] = coco.convert(neighboring_city_ids.search_country, to='iso2')\n",
    "with open('bordering_countries_bordering_cities.json') as json_file:\n",
    "    bordering_country_cities_dict = json.load(json_file)\n",
    "# Convert keys to a series\n",
    "keys_series = pd.Series(list(bordering_country_cities_dict.keys()))\n",
    "converted_keys = coco.convert(keys_series, to='iso2')\n",
    "iso2_border_cities = {key: bordering_country_cities_dict[value] for key, value in zip(converted_keys, bordering_country_cities_dict.keys())}\n",
    "\n",
    "unhcr = pd.read_csv('../../data/clean/unhcr.csv', engine='pyarrow').groupby(['iso_o','iso_d']).agg({'newarrival':'sum','contig':'first','Country_o':'first','Country_d':'first', 'island_o':'first'}).reset_index()\n",
    "iso2_countries = coco.convert(unhcr.Country_o.unique(), to='iso2')\n",
    "\n",
    "city_topic_ids = pd.read_csv('topic_ids/city_topic_id.csv')\n",
    "city_topic_ids['iso2'] = coco.convert(city_topic_ids.search_country, to='iso2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [10:48:29<00:00, 198.52s/it]  \n"
     ]
    }
   ],
   "source": [
    "border_city_trends = pd.DataFrame()\n",
    "for o_country in tqdm(iso2_countries):\n",
    "    o_dict = iso2_border_cities[o_country]\n",
    "    for d_country in o_dict:\n",
    "        topic_mids = neighboring_city_ids[neighboring_city_ids.search_keyword.isin(o_dict[d_country]) & ~neighboring_city_ids.search_keyword.isin(city_topic_ids.search_keyword)].topic_mid\n",
    "        o_trend_d_cities = get_trends_data(o_country, topic_mids, start_date, end_date)\n",
    "        o_trend_d_cities['country_d'] = d_country\n",
    "        o_trend_d_cities['country_o'] = o_country\n",
    "        border_city_trends = pd.concat([border_city_trends, o_trend_d_cities.loc[:, ~o_trend_d_cities.columns.duplicated()]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_city_trends.to_csv('data/2005_topics/bordering_cities_trends_2005.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refugees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
